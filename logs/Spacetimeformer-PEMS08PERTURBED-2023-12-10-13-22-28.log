PEMS08
Trainset:	x-(10700, 12, 170, 3)	y-(10700, 12, 170, 1)
Valset:  	x-(3567, 12, 170, 3)  	y-(3567, 12, 170, 1)
Testset:	x-(3566, 12, 170, 3)	y-(3566, 12, 170, 1)

--------- STAEformer ---------
{
    "num_nodes": 170,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.6,
    "val_size": 0.2,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.001,
    "weight_decay": 0.0015,
    "milestones": [
        25,
        45,
        65
    ],
    "lr_decay_rate": 0.1,
    "batch_size": 16,
    "max_epochs": 50,
    "early_stop": 30,
    "use_cl": false,
    "cl_step_size": 2500,
    "model_args": {
        "num_nodes": 170,
        "in_steps": 12,
        "out_steps": 12,
        "steps_per_day": 288,
        "input_dim": 3,
        "output_dim": 1,
        "input_embedding_dim": 24,
        "tod_embedding_dim": 24,
        "dow_embedding_dim": 24,
        "spatial_embedding_dim": 0,
        "adaptive_embedding_dim": 80,
        "feed_forward_dim": 256,
        "num_heads": 4,
        "num_layers": 3,
        "dropout": 0.1
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
Spacetimeformer                          [16, 12, 170, 1]          170,376
├─Linear: 1-1                            [16, 12, 25840]           13,204,240
├─ModuleList: 1-2                        --                        --
│    └─SelfAttentionLayer: 2-1           [16, 12, 170, 152]        --
│    │    └─AttentionLayer: 3-1          [16, 170, 12, 152]        93,024
│    │    └─Dropout: 3-2                 [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-3               [16, 170, 12, 152]        304
│    │    └─Sequential: 3-4              [16, 170, 12, 152]        78,232
│    │    └─Dropout: 3-5                 [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-6               [16, 170, 12, 152]        304
│    └─SelfAttentionLayer: 2-2           [16, 12, 170, 152]        --
│    │    └─AttentionLayer: 3-7          [16, 170, 12, 152]        93,024
│    │    └─Dropout: 3-8                 [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-9               [16, 170, 12, 152]        304
│    │    └─Sequential: 3-10             [16, 170, 12, 152]        78,232
│    │    └─Dropout: 3-11                [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-12              [16, 170, 12, 152]        304
│    └─SelfAttentionLayer: 2-3           [16, 12, 170, 152]        --
│    │    └─AttentionLayer: 3-13         [16, 170, 12, 152]        93,024
│    │    └─Dropout: 3-14                [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-15              [16, 170, 12, 152]        304
│    │    └─Sequential: 3-16             [16, 170, 12, 152]        78,232
│    │    └─Dropout: 3-17                [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-18              [16, 170, 12, 152]        304
├─ModuleList: 1-3                        --                        --
│    └─SelfAttentionLayer: 2-4           [16, 12, 170, 152]        --
│    │    └─AttentionLayer: 3-19         [16, 12, 170, 152]        93,024
│    │    └─Dropout: 3-20                [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-21              [16, 12, 170, 152]        304
│    │    └─Sequential: 3-22             [16, 12, 170, 152]        78,232
│    │    └─Dropout: 3-23                [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-24              [16, 12, 170, 152]        304
│    └─SelfAttentionLayer: 2-5           [16, 12, 170, 152]        --
│    │    └─AttentionLayer: 3-25         [16, 12, 170, 152]        93,024
│    │    └─Dropout: 3-26                [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-27              [16, 12, 170, 152]        304
│    │    └─Sequential: 3-28             [16, 12, 170, 152]        78,232
│    │    └─Dropout: 3-29                [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-30              [16, 12, 170, 152]        304
│    └─SelfAttentionLayer: 2-6           [16, 12, 170, 152]        --
│    │    └─AttentionLayer: 3-31         [16, 12, 170, 152]        93,024
│    │    └─Dropout: 3-32                [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-33              [16, 12, 170, 152]        304
│    │    └─Sequential: 3-34             [16, 12, 170, 152]        78,232
│    │    └─Dropout: 3-35                [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-36              [16, 12, 170, 152]        304
├─Linear: 1-4                            [16, 170, 12]             21,900
==========================================================================================
Total params: 14,427,700
Trainable params: 14,427,700
Non-trainable params: 0
Total mult-adds (M): 228.12
==========================================================================================
Input size (MB): 0.39
Forward/backward pass size (MB): 2108.02
Params size (MB): 57.03
Estimated Total Size (MB): 2165.44
==========================================================================================

Loss: HuberLoss

2023-12-10 13:24:24.498905 Epoch 1  	Train Loss = 29.02204 Val Loss = 31.47537
2023-12-10 13:26:17.539192 Epoch 2  	Train Loss = 18.99196 Val Loss = 31.94646
2023-12-10 13:28:10.648403 Epoch 3  	Train Loss = 17.38326 Val Loss = 31.22438
2023-12-10 13:30:03.840206 Epoch 4  	Train Loss = 16.03891 Val Loss = 31.33704
2023-12-10 13:31:57.143385 Epoch 5  	Train Loss = 14.82435 Val Loss = 32.02720
2023-12-10 13:33:50.365186 Epoch 6  	Train Loss = 13.55261 Val Loss = 31.69300
2023-12-10 13:35:43.984261 Epoch 7  	Train Loss = 11.83831 Val Loss = 31.94563
2023-12-10 13:37:37.255186 Epoch 8  	Train Loss = 11.05710 Val Loss = 30.85571
2023-12-10 13:39:30.724389 Epoch 9  	Train Loss = 10.01729 Val Loss = 31.37881
2023-12-10 13:41:23.939444 Epoch 10  	Train Loss = 9.55540 Val Loss = 31.88700
2023-12-10 13:43:17.361637 Epoch 11  	Train Loss = 9.05616 Val Loss = 30.90744
2023-12-10 13:45:11.382073 Epoch 12  	Train Loss = 8.77553 Val Loss = 30.60469
2023-12-10 13:47:05.835076 Epoch 13  	Train Loss = 8.54336 Val Loss = 30.50246
2023-12-10 13:48:59.522851 Epoch 14  	Train Loss = 8.30257 Val Loss = 30.70289
2023-12-10 13:50:52.857624 Epoch 15  	Train Loss = 7.89753 Val Loss = 30.51582
2023-12-10 13:52:46.025750 Epoch 16  	Train Loss = 8.09108 Val Loss = 30.81789
2023-12-10 13:54:39.102631 Epoch 17  	Train Loss = 7.86154 Val Loss = 31.00041
2023-12-10 13:56:32.698652 Epoch 18  	Train Loss = 7.79239 Val Loss = 31.14641
2023-12-10 13:58:26.445615 Epoch 19  	Train Loss = 7.63386 Val Loss = 30.24821
2023-12-10 14:00:20.394992 Epoch 20  	Train Loss = 7.34961 Val Loss = 30.74590
2023-12-10 14:02:14.284011 Epoch 21  	Train Loss = 7.72223 Val Loss = 31.26512
2023-12-10 14:04:07.636236 Epoch 22  	Train Loss = 7.30348 Val Loss = 30.70073
2023-12-10 14:06:00.611611 Epoch 23  	Train Loss = 7.24079 Val Loss = 30.76867
2023-12-10 14:07:53.427402 Epoch 24  	Train Loss = 7.36210 Val Loss = 30.56469
2023-12-10 14:09:46.536035 Epoch 25  	Train Loss = 6.92104 Val Loss = 30.32147
2023-12-10 14:11:40.221821 Epoch 26  	Train Loss = 4.16411 Val Loss = 30.11806
2023-12-10 14:13:33.940937 Epoch 27  	Train Loss = 2.77490 Val Loss = 30.40458
2023-12-10 14:15:27.650480 Epoch 28  	Train Loss = 2.40593 Val Loss = 30.42109
2023-12-10 14:17:21.482135 Epoch 29  	Train Loss = 2.21750 Val Loss = 30.41446
2023-12-10 14:19:15.383534 Epoch 30  	Train Loss = 2.09483 Val Loss = 30.47099
2023-12-10 14:21:09.119972 Epoch 31  	Train Loss = 2.00046 Val Loss = 30.39866
2023-12-10 14:23:02.863117 Epoch 32  	Train Loss = 1.93168 Val Loss = 30.46248
2023-12-10 14:24:55.917629 Epoch 33  	Train Loss = 1.87281 Val Loss = 30.46103
2023-12-10 14:26:48.643194 Epoch 34  	Train Loss = 1.83533 Val Loss = 30.62309
2023-12-10 14:28:41.313610 Epoch 35  	Train Loss = 1.79728 Val Loss = 30.50999
2023-12-10 14:30:34.056237 Epoch 36  	Train Loss = 1.77744 Val Loss = 30.51405
2023-12-10 14:32:27.418549 Epoch 37  	Train Loss = 1.74341 Val Loss = 30.55404
2023-12-10 14:34:21.046265 Epoch 38  	Train Loss = 1.73551 Val Loss = 30.52683
2023-12-10 14:36:14.623277 Epoch 39  	Train Loss = 1.71416 Val Loss = 30.57802
2023-12-10 14:38:08.342116 Epoch 40  	Train Loss = 1.70061 Val Loss = 30.49772
2023-12-10 14:40:02.125997 Epoch 41  	Train Loss = 1.67564 Val Loss = 30.69353
2023-12-10 14:41:55.899561 Epoch 42  	Train Loss = 1.65880 Val Loss = 30.49415
2023-12-10 14:43:49.591747 Epoch 43  	Train Loss = 1.67417 Val Loss = 30.35026
2023-12-10 14:45:43.281047 Epoch 44  	Train Loss = 1.65332 Val Loss = 30.50368
2023-12-10 14:47:36.944191 Epoch 45  	Train Loss = 1.63898 Val Loss = 30.45804
2023-12-10 14:49:30.838689 Epoch 46  	Train Loss = 1.19752 Val Loss = 30.48300
2023-12-10 14:51:24.731434 Epoch 47  	Train Loss = 1.00899 Val Loss = 30.48979
2023-12-10 14:53:18.551293 Epoch 48  	Train Loss = 0.97026 Val Loss = 30.50452
2023-12-10 14:55:12.431122 Epoch 49  	Train Loss = 0.95230 Val Loss = 30.51393
2023-12-10 14:57:06.322712 Epoch 50  	Train Loss = 0.93944 Val Loss = 30.51734
Early stopping at epoch: 50
Best at epoch 26:
Train Loss = 4.16411
Train RMSE = 1.41475, MAE = 0.86110, MAPE = 1.63989
Val Loss = 30.11806
Val RMSE = 48.01218, MAE = 31.00934, MAPE = 72.59979
Saved Model: ../saved_models/STAEformer-PEMS08-2023-12-10-13-22-28.pt
--------- Test ---------
All Steps RMSE = 44.63800, MAE = 30.10142, MAPE = 32.41994
Step 1 RMSE = 42.02646, MAE = 28.18470, MAPE = 31.59348
Step 2 RMSE = 42.49020, MAE = 28.52628, MAPE = 31.54761
Step 3 RMSE = 42.93185, MAE = 28.84498, MAPE = 31.57088
Step 4 RMSE = 43.34600, MAE = 29.15498, MAPE = 31.80477
Step 5 RMSE = 43.78704, MAE = 29.48325, MAPE = 31.98579
Step 6 RMSE = 44.25035, MAE = 29.81938, MAPE = 32.15175
Step 7 RMSE = 44.73091, MAE = 30.17136, MAPE = 32.28938
Step 8 RMSE = 45.21378, MAE = 30.53387, MAPE = 32.62280
Step 9 RMSE = 45.72595, MAE = 30.92321, MAPE = 32.82037
Step 10 RMSE = 46.24959, MAE = 31.33247, MAPE = 33.08684
Step 11 RMSE = 46.84674, MAE = 31.80309, MAPE = 33.51194
Step 12 RMSE = 47.66480, MAE = 32.43936, MAPE = 34.05377
Inference time: 10.84 s
