PEMS08
Trainset:	x-(10700, 12, 170, 3)	y-(10700, 12, 170, 1)
Valset:  	x-(3567, 12, 170, 3)  	y-(3567, 12, 170, 1)
Testset:	x-(3566, 12, 170, 3)	y-(3566, 12, 170, 1)

--------- STAEformer ---------
{
    "num_nodes": 170,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.6,
    "val_size": 0.2,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.001,
    "weight_decay": 0.0015,
    "milestones": [
        25,
        45,
        65
    ],
    "lr_decay_rate": 0.1,
    "batch_size": 16,
    "max_epochs": 50,
    "early_stop": 30,
    "use_cl": false,
    "cl_step_size": 2500,
    "model_args": {
        "num_nodes": 170,
        "in_steps": 12,
        "out_steps": 12,
        "steps_per_day": 288,
        "input_dim": 3,
        "output_dim": 1,
        "input_embedding_dim": 24,
        "tod_embedding_dim": 24,
        "dow_embedding_dim": 24,
        "spatial_embedding_dim": 0,
        "adaptive_embedding_dim": 80,
        "feed_forward_dim": 256,
        "num_heads": 4,
        "num_layers": 3,
        "dropout": 0.1
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
Spacetimeformer                          [16, 12, 170, 1]          170,376
├─Linear: 1-1                            [16, 12, 25840]           13,204,240
├─ModuleList: 1-2                        --                        --
│    └─SelfAttentionLayer: 2-1           [16, 12, 170, 152]        --
│    │    └─AttentionLayer: 3-1          [16, 170, 12, 152]        93,024
│    │    └─Dropout: 3-2                 [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-3               [16, 170, 12, 152]        304
│    │    └─Sequential: 3-4              [16, 170, 12, 152]        78,232
│    │    └─Dropout: 3-5                 [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-6               [16, 170, 12, 152]        304
│    └─SelfAttentionLayer: 2-2           [16, 12, 170, 152]        --
│    │    └─AttentionLayer: 3-7          [16, 170, 12, 152]        93,024
│    │    └─Dropout: 3-8                 [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-9               [16, 170, 12, 152]        304
│    │    └─Sequential: 3-10             [16, 170, 12, 152]        78,232
│    │    └─Dropout: 3-11                [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-12              [16, 170, 12, 152]        304
│    └─SelfAttentionLayer: 2-3           [16, 12, 170, 152]        --
│    │    └─AttentionLayer: 3-13         [16, 170, 12, 152]        93,024
│    │    └─Dropout: 3-14                [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-15              [16, 170, 12, 152]        304
│    │    └─Sequential: 3-16             [16, 170, 12, 152]        78,232
│    │    └─Dropout: 3-17                [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-18              [16, 170, 12, 152]        304
├─ModuleList: 1-3                        --                        --
│    └─SelfAttentionLayer: 2-4           [16, 12, 170, 152]        --
│    │    └─AttentionLayer: 3-19         [16, 12, 170, 152]        93,024
│    │    └─Dropout: 3-20                [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-21              [16, 12, 170, 152]        304
│    │    └─Sequential: 3-22             [16, 12, 170, 152]        78,232
│    │    └─Dropout: 3-23                [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-24              [16, 12, 170, 152]        304
│    └─SelfAttentionLayer: 2-5           [16, 12, 170, 152]        --
│    │    └─AttentionLayer: 3-25         [16, 12, 170, 152]        93,024
│    │    └─Dropout: 3-26                [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-27              [16, 12, 170, 152]        304
│    │    └─Sequential: 3-28             [16, 12, 170, 152]        78,232
│    │    └─Dropout: 3-29                [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-30              [16, 12, 170, 152]        304
│    └─SelfAttentionLayer: 2-6           [16, 12, 170, 152]        --
│    │    └─AttentionLayer: 3-31         [16, 12, 170, 152]        93,024
│    │    └─Dropout: 3-32                [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-33              [16, 12, 170, 152]        304
│    │    └─Sequential: 3-34             [16, 12, 170, 152]        78,232
│    │    └─Dropout: 3-35                [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-36              [16, 12, 170, 152]        304
├─Linear: 1-4                            [16, 170, 12]             21,900
==========================================================================================
Total params: 14,427,700
Trainable params: 14,427,700
Non-trainable params: 0
Total mult-adds (M): 228.12
==========================================================================================
Input size (MB): 0.39
Forward/backward pass size (MB): 2108.02
Params size (MB): 57.03
Estimated Total Size (MB): 2165.44
==========================================================================================

Loss: HuberLoss

2023-12-10 20:48:22.153910 Epoch 1  	Train Loss = 29.59790 Val Loss = 24.16559
2023-12-10 20:50:14.530747 Epoch 2  	Train Loss = 21.30297 Val Loss = 23.76212
2023-12-10 20:52:06.530274 Epoch 3  	Train Loss = 19.53174 Val Loss = 21.71964
2023-12-10 20:53:58.125229 Epoch 4  	Train Loss = 18.47406 Val Loss = 21.49390
2023-12-10 20:55:49.478577 Epoch 5  	Train Loss = 17.98662 Val Loss = 20.35875
2023-12-10 20:57:40.661207 Epoch 6  	Train Loss = 17.41894 Val Loss = 20.34965
2023-12-10 20:59:31.835916 Epoch 7  	Train Loss = 17.16387 Val Loss = 19.95819
2023-12-10 21:01:23.071371 Epoch 8  	Train Loss = 16.79306 Val Loss = 19.63527
2023-12-10 21:03:14.358206 Epoch 9  	Train Loss = 16.70015 Val Loss = 19.14424
2023-12-10 21:05:05.735232 Epoch 10  	Train Loss = 16.54575 Val Loss = 19.49371
2023-12-10 21:06:57.056211 Epoch 11  	Train Loss = 16.36680 Val Loss = 18.85766
2023-12-10 21:08:48.241268 Epoch 12  	Train Loss = 16.20868 Val Loss = 19.04023
2023-12-10 21:10:39.314092 Epoch 13  	Train Loss = 16.10381 Val Loss = 19.16333
2023-12-10 21:12:30.244348 Epoch 14  	Train Loss = 16.03111 Val Loss = 18.69561
2023-12-10 21:14:21.188394 Epoch 15  	Train Loss = 15.98294 Val Loss = 18.50399
2023-12-10 21:16:12.078434 Epoch 16  	Train Loss = 15.90590 Val Loss = 18.60442
2023-12-10 21:18:03.166725 Epoch 17  	Train Loss = 15.85314 Val Loss = 18.58837
2023-12-10 21:19:54.144490 Epoch 18  	Train Loss = 15.86436 Val Loss = 18.84594
2023-12-10 21:21:45.164439 Epoch 19  	Train Loss = 15.74631 Val Loss = 18.52780
2023-12-10 21:23:36.019221 Epoch 20  	Train Loss = 15.72387 Val Loss = 18.29090
2023-12-10 21:25:26.930755 Epoch 21  	Train Loss = 15.58876 Val Loss = 18.98682
2023-12-10 21:27:17.801438 Epoch 22  	Train Loss = 15.63712 Val Loss = 18.52548
2023-12-10 21:29:08.719541 Epoch 23  	Train Loss = 15.55444 Val Loss = 18.78183
2023-12-10 21:30:59.702975 Epoch 24  	Train Loss = 15.56607 Val Loss = 18.35971
2023-12-10 21:32:50.663913 Epoch 25  	Train Loss = 15.54894 Val Loss = 18.66411
2023-12-10 21:34:41.640340 Epoch 26  	Train Loss = 14.30095 Val Loss = 17.34711
2023-12-10 21:36:32.749130 Epoch 27  	Train Loss = 14.16455 Val Loss = 17.31819
2023-12-10 21:38:24.100204 Epoch 28  	Train Loss = 14.11385 Val Loss = 17.11704
2023-12-10 21:40:15.733866 Epoch 29  	Train Loss = 14.06843 Val Loss = 17.28979
2023-12-10 21:42:07.194091 Epoch 30  	Train Loss = 14.03544 Val Loss = 17.24246
2023-12-10 21:43:58.504146 Epoch 31  	Train Loss = 13.99318 Val Loss = 17.14367
2023-12-10 21:45:49.756373 Epoch 32  	Train Loss = 13.95941 Val Loss = 17.15445
2023-12-10 21:47:40.580628 Epoch 33  	Train Loss = 13.92398 Val Loss = 17.17279
2023-12-10 21:49:31.278425 Epoch 34  	Train Loss = 13.89479 Val Loss = 17.19349
2023-12-10 21:51:22.059003 Epoch 35  	Train Loss = 13.86497 Val Loss = 17.26630
2023-12-10 21:53:13.050488 Epoch 36  	Train Loss = 13.84101 Val Loss = 17.19431
2023-12-10 21:55:04.557453 Epoch 37  	Train Loss = 13.81388 Val Loss = 17.22965
2023-12-10 21:56:55.733748 Epoch 38  	Train Loss = 13.79006 Val Loss = 17.21499
2023-12-10 21:58:46.553139 Epoch 39  	Train Loss = 13.76015 Val Loss = 17.16771
2023-12-10 22:00:37.397952 Epoch 40  	Train Loss = 13.74894 Val Loss = 17.13326
2023-12-10 22:02:28.475375 Epoch 41  	Train Loss = 13.72436 Val Loss = 17.18934
2023-12-10 22:04:19.368425 Epoch 42  	Train Loss = 13.70203 Val Loss = 17.09423
2023-12-10 22:06:10.024405 Epoch 43  	Train Loss = 13.67896 Val Loss = 17.19280
2023-12-10 22:08:00.844860 Epoch 44  	Train Loss = 13.66060 Val Loss = 17.25672
2023-12-10 22:09:51.938185 Epoch 45  	Train Loss = 13.64803 Val Loss = 17.23097
2023-12-10 22:11:42.956098 Epoch 46  	Train Loss = 13.44251 Val Loss = 17.01614
2023-12-10 22:13:34.032108 Epoch 47  	Train Loss = 13.41728 Val Loss = 17.03153
2023-12-10 22:15:24.945280 Epoch 48  	Train Loss = 13.41135 Val Loss = 17.03387
2023-12-10 22:17:15.948932 Epoch 49  	Train Loss = 13.40509 Val Loss = 17.06752
2023-12-10 22:19:06.938384 Epoch 50  	Train Loss = 13.39987 Val Loss = 17.04363
Early stopping at epoch: 50
Best at epoch 46:
Train Loss = 13.44251
Train RMSE = 22.48986, MAE = 13.74496, MAPE = 28.56712
Val Loss = 17.01614
Val RMSE = 29.36041, MAE = 17.47888, MAPE = 15.14841
Saved Model: ../saved_models/STAEformer-PEMS08-2023-12-10-20-46-27.pt
--------- Test ---------
All Steps RMSE = 27.43269, MAE = 17.16214, MAPE = 10.95698
Step 1 RMSE = 25.73117, MAE = 16.41139, MAPE = 10.52883
Step 2 RMSE = 26.19073, MAE = 16.61427, MAPE = 10.62839
Step 3 RMSE = 26.52107, MAE = 16.75626, MAPE = 10.71380
Step 4 RMSE = 26.82676, MAE = 16.87232, MAPE = 10.78673
Step 5 RMSE = 27.08729, MAE = 16.97390, MAPE = 10.84224
Step 6 RMSE = 27.36098, MAE = 17.09247, MAPE = 10.90362
Step 7 RMSE = 27.62285, MAE = 17.22015, MAPE = 11.00136
Step 8 RMSE = 27.87630, MAE = 17.34593, MAPE = 11.05571
Step 9 RMSE = 28.08655, MAE = 17.46087, MAPE = 11.15009
Step 10 RMSE = 28.30974, MAE = 17.57356, MAPE = 11.18561
Step 11 RMSE = 28.54695, MAE = 17.71577, MAPE = 11.27829
Step 12 RMSE = 28.84200, MAE = 17.90880, MAPE = 11.40904
Inference time: 10.57 s
