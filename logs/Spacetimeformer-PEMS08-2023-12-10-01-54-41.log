PEMS08
Trainset:	x-(10700, 12, 170, 3)	y-(10700, 12, 170, 1)
Valset:  	x-(3567, 12, 170, 3)  	y-(3567, 12, 170, 1)
Testset:	x-(3566, 12, 170, 3)	y-(3566, 12, 170, 1)

--------- STAEformer ---------
{
    "num_nodes": 170,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.6,
    "val_size": 0.2,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.001,
    "weight_decay": 0.0015,
    "milestones": [
        25,
        45,
        65
    ],
    "lr_decay_rate": 0.1,
    "batch_size": 16,
    "max_epochs": 50,
    "early_stop": 30,
    "use_cl": false,
    "cl_step_size": 2500,
    "model_args": {
        "num_nodes": 170,
        "in_steps": 12,
        "out_steps": 12,
        "steps_per_day": 288,
        "input_dim": 3,
        "output_dim": 1,
        "input_embedding_dim": 24,
        "tod_embedding_dim": 24,
        "dow_embedding_dim": 24,
        "spatial_embedding_dim": 0,
        "adaptive_embedding_dim": 80,
        "feed_forward_dim": 256,
        "num_heads": 4,
        "num_layers": 3,
        "dropout": 0.1
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
Spacetimeformer                          [16, 12, 170, 1]          170,376
├─Linear: 1-1                            [16, 12, 25840]           13,204,240
├─ModuleList: 1-2                        --                        --
│    └─SelfAttentionLayer: 2-1           [16, 12, 170, 152]        --
│    │    └─AttentionLayer: 3-1          [16, 170, 12, 152]        93,024
│    │    └─Dropout: 3-2                 [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-3               [16, 170, 12, 152]        304
│    │    └─Sequential: 3-4              [16, 170, 12, 152]        78,232
│    │    └─Dropout: 3-5                 [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-6               [16, 170, 12, 152]        304
│    └─SelfAttentionLayer: 2-2           [16, 12, 170, 152]        --
│    │    └─AttentionLayer: 3-7          [16, 170, 12, 152]        93,024
│    │    └─Dropout: 3-8                 [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-9               [16, 170, 12, 152]        304
│    │    └─Sequential: 3-10             [16, 170, 12, 152]        78,232
│    │    └─Dropout: 3-11                [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-12              [16, 170, 12, 152]        304
│    └─SelfAttentionLayer: 2-3           [16, 12, 170, 152]        --
│    │    └─AttentionLayer: 3-13         [16, 170, 12, 152]        93,024
│    │    └─Dropout: 3-14                [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-15              [16, 170, 12, 152]        304
│    │    └─Sequential: 3-16             [16, 170, 12, 152]        78,232
│    │    └─Dropout: 3-17                [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-18              [16, 170, 12, 152]        304
├─ModuleList: 1-3                        --                        --
│    └─SelfAttentionLayer: 2-4           [16, 12, 170, 152]        --
│    │    └─AttentionLayer: 3-19         [16, 12, 170, 152]        93,024
│    │    └─Dropout: 3-20                [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-21              [16, 12, 170, 152]        304
│    │    └─Sequential: 3-22             [16, 12, 170, 152]        78,232
│    │    └─Dropout: 3-23                [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-24              [16, 12, 170, 152]        304
│    └─SelfAttentionLayer: 2-5           [16, 12, 170, 152]        --
│    │    └─AttentionLayer: 3-25         [16, 12, 170, 152]        93,024
│    │    └─Dropout: 3-26                [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-27              [16, 12, 170, 152]        304
│    │    └─Sequential: 3-28             [16, 12, 170, 152]        78,232
│    │    └─Dropout: 3-29                [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-30              [16, 12, 170, 152]        304
│    └─SelfAttentionLayer: 2-6           [16, 12, 170, 152]        --
│    │    └─AttentionLayer: 3-31         [16, 12, 170, 152]        93,024
│    │    └─Dropout: 3-32                [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-33              [16, 12, 170, 152]        304
│    │    └─Sequential: 3-34             [16, 12, 170, 152]        78,232
│    │    └─Dropout: 3-35                [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-36              [16, 12, 170, 152]        304
├─Linear: 1-4                            [16, 170, 12]             21,900
==========================================================================================
Total params: 14,427,700
Trainable params: 14,427,700
Non-trainable params: 0
Total mult-adds (M): 228.12
==========================================================================================
Input size (MB): 0.39
Forward/backward pass size (MB): 2108.02
Params size (MB): 57.03
Estimated Total Size (MB): 2165.44
==========================================================================================

Loss: HuberLoss

2023-12-10 01:56:35.564189 Epoch 1  	Train Loss = 28.21589 Val Loss = 25.61714
2023-12-10 01:58:27.460817 Epoch 2  	Train Loss = 20.28515 Val Loss = 22.37620
2023-12-10 02:00:19.357133 Epoch 3  	Train Loss = 18.84702 Val Loss = 22.37995
2023-12-10 02:02:11.294603 Epoch 4  	Train Loss = 17.91172 Val Loss = 20.47645
2023-12-10 02:04:03.301877 Epoch 5  	Train Loss = 17.10932 Val Loss = 21.08565
2023-12-10 02:05:55.241973 Epoch 6  	Train Loss = 16.77421 Val Loss = 20.79234
2023-12-10 02:07:47.125603 Epoch 7  	Train Loss = 16.33103 Val Loss = 19.64599
2023-12-10 02:09:39.037598 Epoch 8  	Train Loss = 16.12290 Val Loss = 20.12572
2023-12-10 02:11:30.941908 Epoch 9  	Train Loss = 15.91323 Val Loss = 19.06007
2023-12-10 02:13:22.677935 Epoch 10  	Train Loss = 15.83604 Val Loss = 19.84468
2023-12-10 02:15:14.121088 Epoch 11  	Train Loss = 15.55666 Val Loss = 19.26062
2023-12-10 02:17:05.755862 Epoch 12  	Train Loss = 15.45958 Val Loss = 18.97970
2023-12-10 02:19:01.851077 Epoch 13  	Train Loss = 15.37212 Val Loss = 18.93436
2023-12-10 02:22:39.187159 Epoch 14  	Train Loss = 15.26945 Val Loss = 18.83755
2023-12-10 02:24:39.384610 Epoch 15  	Train Loss = 15.27049 Val Loss = 19.00094
2023-12-10 02:26:30.654244 Epoch 16  	Train Loss = 15.08056 Val Loss = 18.69691
2023-12-10 02:28:22.073663 Epoch 17  	Train Loss = 15.06456 Val Loss = 18.25352
2023-12-10 02:30:13.563000 Epoch 18  	Train Loss = 15.09089 Val Loss = 18.78949
2023-12-10 02:32:05.199134 Epoch 19  	Train Loss = 14.98005 Val Loss = 17.97676
2023-12-10 02:33:56.677697 Epoch 20  	Train Loss = 14.98353 Val Loss = 19.17144
2023-12-10 02:35:48.262371 Epoch 21  	Train Loss = 14.90091 Val Loss = 18.56206
2023-12-10 02:37:39.874970 Epoch 22  	Train Loss = 14.89308 Val Loss = 19.20020
2023-12-10 02:39:31.451647 Epoch 23  	Train Loss = 14.78524 Val Loss = 18.51793
2023-12-10 02:41:23.044147 Epoch 24  	Train Loss = 14.83006 Val Loss = 18.75756
2023-12-10 02:43:14.742214 Epoch 25  	Train Loss = 14.74105 Val Loss = 18.18589
2023-12-10 02:45:06.562851 Epoch 26  	Train Loss = 13.44183 Val Loss = 17.01583
2023-12-10 02:46:58.284733 Epoch 27  	Train Loss = 13.28724 Val Loss = 17.01177
2023-12-10 02:48:49.922080 Epoch 28  	Train Loss = 13.23038 Val Loss = 17.02208
2023-12-10 02:50:41.554132 Epoch 29  	Train Loss = 13.17707 Val Loss = 17.06156
2023-12-10 02:52:32.639908 Epoch 30  	Train Loss = 13.13276 Val Loss = 16.99243
2023-12-10 02:54:23.822898 Epoch 31  	Train Loss = 13.08678 Val Loss = 16.98493
2023-12-10 02:56:15.095053 Epoch 32  	Train Loss = 13.05233 Val Loss = 16.99715
2023-12-10 02:58:06.091644 Epoch 33  	Train Loss = 13.01203 Val Loss = 17.00801
2023-12-10 02:59:56.953297 Epoch 34  	Train Loss = 12.97674 Val Loss = 17.02411
2023-12-10 03:01:47.719668 Epoch 35  	Train Loss = 12.94907 Val Loss = 16.92766
2023-12-10 03:03:38.469996 Epoch 36  	Train Loss = 12.91709 Val Loss = 17.00059
2023-12-10 03:05:29.221633 Epoch 37  	Train Loss = 12.88516 Val Loss = 17.03371
2023-12-10 03:07:19.937031 Epoch 38  	Train Loss = 12.85178 Val Loss = 16.92774
2023-12-10 03:09:10.931539 Epoch 39  	Train Loss = 12.82456 Val Loss = 17.03612
2023-12-10 03:11:02.350424 Epoch 40  	Train Loss = 12.79979 Val Loss = 16.89954
2023-12-10 03:12:53.928226 Epoch 41  	Train Loss = 12.76919 Val Loss = 16.95940
2023-12-10 03:14:45.492005 Epoch 42  	Train Loss = 12.74934 Val Loss = 17.02011
2023-12-10 03:16:37.027739 Epoch 43  	Train Loss = 12.72252 Val Loss = 16.98400
2023-12-10 03:18:28.533425 Epoch 44  	Train Loss = 12.69254 Val Loss = 17.04564
2023-12-10 03:20:20.081887 Epoch 45  	Train Loss = 12.67352 Val Loss = 17.01249
2023-12-10 03:22:11.617798 Epoch 46  	Train Loss = 12.44894 Val Loss = 16.85715
2023-12-10 03:24:03.213860 Epoch 47  	Train Loss = 12.42218 Val Loss = 16.85528
2023-12-10 03:25:54.716960 Epoch 48  	Train Loss = 12.41000 Val Loss = 16.87451
2023-12-10 03:27:46.074001 Epoch 49  	Train Loss = 12.40418 Val Loss = 16.87112
2023-12-10 03:29:37.526500 Epoch 50  	Train Loss = 12.39563 Val Loss = 16.89048
Early stopping at epoch: 50
Best at epoch 47:
Train Loss = 12.42218
Train RMSE = 21.73675, MAE = 12.69881, MAPE = 8.37173
Val Loss = 16.85528
Val RMSE = 29.24080, MAE = 17.32753, MAPE = 15.20936
Saved Model: ../saved_models/STAEformer-PEMS08-2023-12-10-01-54-41.pt
--------- Test ---------
All Steps RMSE = 27.45882, MAE = 17.19757, MAPE = 10.90676
Step 1 RMSE = 25.88984, MAE = 16.50720, MAPE = 10.53237
Step 2 RMSE = 26.31576, MAE = 16.69126, MAPE = 10.62335
Step 3 RMSE = 26.63624, MAE = 16.82465, MAPE = 10.69084
Step 4 RMSE = 26.92644, MAE = 16.92711, MAPE = 10.77202
Step 5 RMSE = 27.16147, MAE = 17.02523, MAPE = 10.81104
Step 6 RMSE = 27.38319, MAE = 17.12013, MAPE = 10.84320
Step 7 RMSE = 27.64031, MAE = 17.24953, MAPE = 10.93644
Step 8 RMSE = 27.85241, MAE = 17.36022, MAPE = 10.99006
Step 9 RMSE = 28.05225, MAE = 17.46537, MAPE = 11.04603
Step 10 RMSE = 28.23508, MAE = 17.56599, MAPE = 11.09852
Step 11 RMSE = 28.47916, MAE = 17.71705, MAPE = 11.19647
Step 12 RMSE = 28.77468, MAE = 17.91712, MAPE = 11.34084
Inference time: 10.64 s
