PEMS08
Trainset:	x-(10700, 12, 170, 3)	y-(10700, 12, 170, 1)
Valset:  	x-(3567, 12, 170, 3)  	y-(3567, 12, 170, 1)
Testset:	x-(3566, 12, 170, 3)	y-(3566, 12, 170, 1)

--------- STAEformer ---------
{
    "num_nodes": 170,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.6,
    "val_size": 0.2,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.001,
    "weight_decay": 0.0015,
    "milestones": [
        25,
        45,
        65
    ],
    "lr_decay_rate": 0.1,
    "batch_size": 16,
    "max_epochs": 50,
    "early_stop": 30,
    "use_cl": false,
    "cl_step_size": 2500,
    "model_args": {
        "num_nodes": 170,
        "in_steps": 12,
        "out_steps": 12,
        "steps_per_day": 288,
        "input_dim": 3,
        "output_dim": 1,
        "input_embedding_dim": 24,
        "tod_embedding_dim": 24,
        "dow_embedding_dim": 24,
        "spatial_embedding_dim": 0,
        "adaptive_embedding_dim": 80,
        "feed_forward_dim": 256,
        "num_heads": 4,
        "num_layers": 3,
        "dropout": 0.1
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
STAEformer                               [16, 12, 170, 1]          163,200
├─Linear: 1-1                            [16, 12, 170, 24]         96
├─Embedding: 1-2                         [16, 12, 170, 24]         6,912
├─Embedding: 1-3                         [16, 12, 170, 24]         168
├─ModuleList: 1-4                        --                        --
│    └─SelfAttentionLayer: 2-1           [16, 12, 170, 152]        --
│    │    └─AttentionLayer: 3-1          [16, 170, 12, 152]        93,024
│    │    └─Dropout: 3-2                 [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-3               [16, 170, 12, 152]        304
│    │    └─Sequential: 3-4              [16, 170, 12, 152]        78,232
│    │    └─Dropout: 3-5                 [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-6               [16, 170, 12, 152]        304
│    └─SelfAttentionLayer: 2-2           [16, 12, 170, 152]        --
│    │    └─AttentionLayer: 3-7          [16, 170, 12, 152]        93,024
│    │    └─Dropout: 3-8                 [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-9               [16, 170, 12, 152]        304
│    │    └─Sequential: 3-10             [16, 170, 12, 152]        78,232
│    │    └─Dropout: 3-11                [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-12              [16, 170, 12, 152]        304
│    └─SelfAttentionLayer: 2-3           [16, 12, 170, 152]        --
│    │    └─AttentionLayer: 3-13         [16, 170, 12, 152]        93,024
│    │    └─Dropout: 3-14                [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-15              [16, 170, 12, 152]        304
│    │    └─Sequential: 3-16             [16, 170, 12, 152]        78,232
│    │    └─Dropout: 3-17                [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-18              [16, 170, 12, 152]        304
├─ModuleList: 1-5                        --                        --
│    └─SelfAttentionLayer: 2-4           [16, 12, 170, 152]        --
│    │    └─AttentionLayer: 3-19         [16, 12, 170, 152]        93,024
│    │    └─Dropout: 3-20                [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-21              [16, 12, 170, 152]        304
│    │    └─Sequential: 3-22             [16, 12, 170, 152]        78,232
│    │    └─Dropout: 3-23                [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-24              [16, 12, 170, 152]        304
│    └─SelfAttentionLayer: 2-5           [16, 12, 170, 152]        --
│    │    └─AttentionLayer: 3-25         [16, 12, 170, 152]        93,024
│    │    └─Dropout: 3-26                [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-27              [16, 12, 170, 152]        304
│    │    └─Sequential: 3-28             [16, 12, 170, 152]        78,232
│    │    └─Dropout: 3-29                [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-30              [16, 12, 170, 152]        304
│    └─SelfAttentionLayer: 2-6           [16, 12, 170, 152]        --
│    │    └─AttentionLayer: 3-31         [16, 12, 170, 152]        93,024
│    │    └─Dropout: 3-32                [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-33              [16, 12, 170, 152]        304
│    │    └─Sequential: 3-34             [16, 12, 170, 152]        78,232
│    │    └─Dropout: 3-35                [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-36              [16, 12, 170, 152]        304
├─Linear: 1-6                            [16, 170, 12]             21,900
==========================================================================================
Total params: 1,223,460
Trainable params: 1,223,460
Non-trainable params: 0
Total mult-adds (M): 16.96
==========================================================================================
Input size (MB): 0.39
Forward/backward pass size (MB): 2087.13
Params size (MB): 4.24
Estimated Total Size (MB): 2091.76
==========================================================================================

Loss: HuberLoss

2023-12-10 18:50:33.726429 Epoch 1  	Train Loss = 26.73409 Val Loss = 20.55360
2023-12-10 18:52:24.974990 Epoch 2  	Train Loss = 19.39974 Val Loss = 18.13819
2023-12-10 18:54:15.784515 Epoch 3  	Train Loss = 18.33249 Val Loss = 17.06407
2023-12-10 18:56:06.186795 Epoch 4  	Train Loss = 17.34739 Val Loss = 16.09481
2023-12-10 18:57:56.586228 Epoch 5  	Train Loss = 16.94666 Val Loss = 16.12550
2023-12-10 18:59:47.197473 Epoch 6  	Train Loss = 16.49086 Val Loss = 15.17803
2023-12-10 19:01:38.918905 Epoch 7  	Train Loss = 16.13459 Val Loss = 15.43824
2023-12-10 19:03:29.350421 Epoch 8  	Train Loss = 16.02816 Val Loss = 15.07625
2023-12-10 19:05:17.812571 Epoch 9  	Train Loss = 15.69172 Val Loss = 15.73712
2023-12-10 19:07:06.071001 Epoch 10  	Train Loss = 15.47476 Val Loss = 15.27832
2023-12-10 19:08:54.510481 Epoch 11  	Train Loss = 15.45587 Val Loss = 14.67222
2023-12-10 19:10:42.941373 Epoch 12  	Train Loss = 15.25694 Val Loss = 14.61399
2023-12-10 19:12:31.946075 Epoch 13  	Train Loss = 15.06162 Val Loss = 14.68328
2023-12-10 19:14:20.334537 Epoch 14  	Train Loss = 14.91619 Val Loss = 14.75436
2023-12-10 19:16:08.986612 Epoch 15  	Train Loss = 14.85386 Val Loss = 14.62403
2023-12-10 19:17:58.368890 Epoch 16  	Train Loss = 14.81867 Val Loss = 13.93021
2023-12-10 19:19:48.203070 Epoch 17  	Train Loss = 14.66852 Val Loss = 14.28406
2023-12-10 19:21:38.062609 Epoch 18  	Train Loss = 14.57628 Val Loss = 14.01844
2023-12-10 19:23:28.017382 Epoch 19  	Train Loss = 14.48632 Val Loss = 14.35182
2023-12-10 19:25:18.163818 Epoch 20  	Train Loss = 14.47940 Val Loss = 13.89354
2023-12-10 19:27:08.232641 Epoch 21  	Train Loss = 14.38427 Val Loss = 14.08013
2023-12-10 19:28:58.430722 Epoch 22  	Train Loss = 14.33237 Val Loss = 13.87458
2023-12-10 19:30:48.693257 Epoch 23  	Train Loss = 14.25412 Val Loss = 14.63241
2023-12-10 19:32:38.748205 Epoch 24  	Train Loss = 14.22457 Val Loss = 13.96901
2023-12-10 19:34:28.726520 Epoch 25  	Train Loss = 14.12279 Val Loss = 13.90752
2023-12-10 19:36:18.588122 Epoch 26  	Train Loss = 13.64799 Val Loss = 13.37058
2023-12-10 19:38:08.652622 Epoch 27  	Train Loss = 13.58251 Val Loss = 13.35917
2023-12-10 19:39:59.755789 Epoch 28  	Train Loss = 13.56052 Val Loss = 13.38679
2023-12-10 19:41:50.682088 Epoch 29  	Train Loss = 13.54123 Val Loss = 13.37722
2023-12-10 19:43:41.732727 Epoch 30  	Train Loss = 13.52913 Val Loss = 13.44011
2023-12-10 19:45:32.902559 Epoch 31  	Train Loss = 13.50720 Val Loss = 13.38454
2023-12-10 19:47:24.041872 Epoch 32  	Train Loss = 13.49483 Val Loss = 13.40644
2023-12-10 19:49:15.513965 Epoch 33  	Train Loss = 13.48238 Val Loss = 13.38449
2023-12-10 19:51:07.122821 Epoch 34  	Train Loss = 13.46970 Val Loss = 13.43850
2023-12-10 19:52:58.754649 Epoch 35  	Train Loss = 13.45376 Val Loss = 13.36980
2023-12-10 19:54:50.177691 Epoch 36  	Train Loss = 13.44702 Val Loss = 13.42462
2023-12-10 19:56:41.583054 Epoch 37  	Train Loss = 13.43328 Val Loss = 13.44624
2023-12-10 19:58:33.027172 Epoch 38  	Train Loss = 13.42491 Val Loss = 13.56978
2023-12-10 20:00:24.270748 Epoch 39  	Train Loss = 13.41158 Val Loss = 13.48671
2023-12-10 20:02:14.856419 Epoch 40  	Train Loss = 13.40641 Val Loss = 13.45998
2023-12-10 20:04:05.884090 Epoch 41  	Train Loss = 13.39281 Val Loss = 13.43292
2023-12-10 20:05:57.162997 Epoch 42  	Train Loss = 13.38293 Val Loss = 13.42642
2023-12-10 20:07:48.732150 Epoch 43  	Train Loss = 13.37454 Val Loss = 13.46198
2023-12-10 20:09:40.275785 Epoch 44  	Train Loss = 13.36673 Val Loss = 13.43890
2023-12-10 20:11:31.796455 Epoch 45  	Train Loss = 13.35810 Val Loss = 13.46454
2023-12-10 20:13:23.405712 Epoch 46  	Train Loss = 13.29540 Val Loss = 13.38513
2023-12-10 20:15:14.908201 Epoch 47  	Train Loss = 13.28573 Val Loss = 13.39432
2023-12-10 20:17:06.092027 Epoch 48  	Train Loss = 13.28341 Val Loss = 13.38894
2023-12-10 20:18:57.309153 Epoch 49  	Train Loss = 13.28285 Val Loss = 13.39487
2023-12-10 20:20:48.340410 Epoch 50  	Train Loss = 13.27965 Val Loss = 13.39569
Early stopping at epoch: 50
Best at epoch 27:
Train Loss = 13.58251
Train RMSE = 22.64889, MAE = 13.62493, MAPE = 29.63670
Val Loss = 13.35917
Val RMSE = 24.70477, MAE = 13.83207, MAPE = 10.52001
Saved Model: ../saved_models/STAEformer-PEMS08-2023-12-10-18-48-37.pt
--------- Test ---------
All Steps RMSE = 23.63741, MAE = 13.76469, MAPE = 9.11478
Step 1 RMSE = 19.79849, MAE = 12.02488, MAPE = 8.06238
Step 2 RMSE = 20.89112, MAE = 12.45467, MAPE = 8.26428
Step 3 RMSE = 21.77304, MAE = 12.85050, MAPE = 8.48366
Step 4 RMSE = 22.50596, MAE = 13.17772, MAPE = 8.69781
Step 5 RMSE = 23.10520, MAE = 13.45997, MAPE = 8.87578
Step 6 RMSE = 23.65359, MAE = 13.72443, MAPE = 9.04613
Step 7 RMSE = 24.15002, MAE = 13.97075, MAPE = 9.21175
Step 8 RMSE = 24.57970, MAE = 14.19782, MAPE = 9.43683
Step 9 RMSE = 24.96946, MAE = 14.40241, MAPE = 9.51797
Step 10 RMSE = 25.34411, MAE = 14.60846, MAPE = 9.69673
Step 11 RMSE = 25.71527, MAE = 14.83343, MAPE = 9.86821
Step 12 RMSE = 26.22829, MAE = 15.47125, MAPE = 10.21591
Inference time: 10.67 s
